# LLM-based metrics

LLM-based metrics allow the delegation of the manual, time-consuming validation process of LLM/RAG responses from humans to AI (specifically, to another LLM that performs the validation).
Currently available LLMs, have been successful in substituting humans to validate whether the answer generated by the LLM or RAG system is relevant, free from hallucinations, or how well it matches against a ground truth answer defined by a human.
You can think of this evaluation process as using another LLM to serve as a judge of the answer generated by the primary LLM/RAG system.

Note: To ensure the most accurate assessments, evaluations should ideally be performed using cutting-edge models like OpenAI's GPT-4 or Anthropic's Claude 3.5 Sonnet.

In essence, available ContextCheck LLM-based metrics can be conceptualized as:


* **Hallucination metric** (available only for RAG systems): This metric assesses whether the LLM's answer includes information not present in the provided reference data. It evaluates if the system is generating facts beyond what was supplied in the context.

* **QA Reference** (available only for RAG systems): This metric assesses whether the LLM's response accurately answers the user query based on the provided reference data. Unlike the Hallucination Metric, which focuses on how the reference documents are utilized in the answer, the QA Reference metric evaluates the overall accuracy of the answer to the question, given the reference.

* **Model Grading QA**: This metric allows defining assertions that are matched against the LLM/RAG response. Think of it as "regular expressions defined using natural language." For example, given the query "Who are you?", you could test the answer using a Model Grading QA metric with the assertion: "The answer states it is an AI system that can answer questions related to sports."

* **Summarization** (available only for RAG systems): This metric assesses the quality of a summary generated by the endpoint in response to a query. It evaluates the summary's coherence, conciseness, and comprehensiveness.

* **Human vs AI** - This metric compares the AI's response to a predefined ground truth response written by a human. Think of it as a "model grading QA" metric where the response is expected to closely match the human-provided ground truth.


## Understanding the limits of LLM-based evaluation

When working with LLM-based evaluation systems, it's important to understand that while these metrics provide valuable insights, 
they should not be relied upon blindly. Much like how human evaluators can make mistakes or overlook details, these AI-powered metrics 
have limitations and may not always perfectly assess the quality or accuracy of an LLM's output.

Think of these metrics like a human grader with a vast amount of knowledge but also subject to human-like limitations. 
A human judge might overlook a critical detail or make an error in judgment, especially in complex or ambiguous cases. 
Similarly, an LLM-based evaluation system, while powerful and efficient, isn't infallible. 
It can make errors, especially in edge cases or when dealing with nuanced or complex queries.

## Working with LLM evaluation metrics

When evaluating LLM or RAG systems using these metrics, it's essential to understand their strengths and limitations, 
and apply them thoughtfully to ensure accurate and reliable results. We invite you to follow practices listed below:


### Human vs AI

Use this metric when you expect the AI's response to closely mirror a human-written answer in both length and semantic content. It's particularly useful when the desired answer is straightforward and the response should resemble a human's in style and substance. From our experiments, we’ve observed that even smaller models, like GPT-4o-mini, perform adequately with this metric, making it a reliable choice for many scenarios.

### Model Grading QA

If defining an exact answer is challenging or if the expected answer may vary in form, the "Model Grading QA" metric is a better option. This metric allows you to specify detailed expectations about what the answer should or should not include. To use it effectively, think about the type of information you expect in the response and write separate assertions for each key aspect. This approach provides flexibility in evaluation, ensuring that even if the answer isn't a perfect match in wording, it can still meet the intended criteria.

### Hallucination Metric

This metric detects when the AI includes information not present in the references. However, like humans, the AI might misjudge context, flagging valid inferences as hallucinations or missing subtle errors. Interpret results with care, understanding its potential limitations.

### QA Reference

The QA Reference metric checks if the AI’s response correctly answers the query based on the references. However, the AI, like a human, might miss nuances or misinterpret complex information, leading to inaccuracies. Be aware of this when evaluating the results.


## Interpreting metric outputs

After executing LLM-based tests scenarios, you will receive a label indicating the LLM’s evaluation of the output, along with supporting facts (where applicable) that explain the reasoning behind the decision. Here's how to interpret the results:

* **Hallucination, QA Reference, Summary, and Human vs AI Metrics**: The output will include a label such as "factual"/"hallucinated," "correct"/"incorrect," or "good"/"bad." This label shows whether the output passes or fails the test. For example, in the Hallucination metric, if the label is "factual," the test passes; if it’s "hallucinated," the test fails. The accompanying facts provide insights into the LLM's reasoning, explaining how it arrived at its conclusion. This helps users understand the AI’s decision-making process and assess the accuracy of the output.

* **Model Grading QA Metric**: This metric simply produces a label of either "correct" or "incorrect," without additional explanation. The label directly reflects whether the output met the specific criteria outlined in the rubric.


## Implementing metrics in the test scenarios

The implementation is similar to the deterministic metrics. 
Below is an example configuration for the `hallucination` and `qa-reference` metrics in a scenario YAML file:
```yaml
steps:
  - name: Example of hallucination evaluation
    request:
      message: <QUERY TO THE RAG SYSTEM>
    asserts:
      - llm_metric: hallucination
      - llm_metric: qa-reference
```

Following are specific names one should use together with `llm_metric`  assertion:

* `hallucination`

* `qa-reference`

* `model-grading-qa`

* `summarization`

* `human-vs-ai`

Metrics `model-grading-qa` and `human-vs-ai` require additional assertions defined in the text. Refer to details below. 

## 

## Additional considerations

### Hallucination

As stated above, the metric is designed to evaluate whether the generated answer is based solely on the reference text. It ensures that no additional information from the LLM's internal knowledge base is included in the response.

Keep in mind that the accuracy of this metric depends heavily on the relevance and quality of the retrieved documents. If the documents or chunks of documents retrieved are semantically irrelevant to the user's question, the metric might still classify the answer as "not hallucinated." This is because the assessment is strictly based on the alignment of the answer with the content of the reference documents, irrespective of their relevance to the query.

### Model Grading QA examples and syntax

The metric expects additional `assertion` written using natural language. Examples:

```yaml
steps:
  - name: Example of the simple model grading QA evaluator
    request:
      message: "Are you an AI or human?"
    asserts:
      - llm_metric: model-grading-qa
        assertion: Should say it is a human.
  - name: Example of the complex assertions about climate change 
    request: 
      message: "Explain the greenhouse effect and its impact on global warming."
      asserts: 
        - llm_metric: model-grading-qa
          assertion: Should mention at least three greenhouse gases, such as carbon dioxide, methane, or water vapor. 
        - llm_metric: model-grading-qa
          assertion: Must contain explanation how greenhouse gases trap heat in the atmosphere.
        - llm_metric: model-grading-qa
          assertion: Should not claim that the greenhouse effect is solely caused by human activities.
        - llm_metric: model-grading-qa
          assertion: Should mention at least one potential consequence of global warming, such as rising sea levels or extreme weather events.
```

Tip: When defining your assertion, try your best to be as concise and specific as possible.

### Human vs AI example and syntax

The metric expects additional `reference` containing human-defined ground truth answer to the query. Example:

```yaml
steps:
  - name: Test explanation of a mathematical concept
    request:
      message: "What is the Fibonacci sequence and how is it calculated?"
    asserts:
      - llm_metric: human-vs-ai
        reference: "The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1. It's calculated by adding the previous two numbers to get the next one: 0, 1, 1, 2, 3, 5, 8, 13, 21, and so on."
```
