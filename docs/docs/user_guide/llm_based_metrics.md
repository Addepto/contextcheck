# LLM-based metrics

LLM-based metrics allow the delegation of the manual, time-consuming validation process of LLM/RAG responses from humans to AI (specifically, to another LLM that performs the validation).
Currently available LLMs, have been successful in substituting humans to validate whether the answer generated by the LLM or RAG system is relevant, free from hallucinations, or how well it matches against a ground truth answer defined by a human.
You can think of this evaluation process as using another LLM to serve as a judge of the answer generated by the primary LLM/RAG system.

Note: To ensure the most accurate assessments, evaluations should ideally be performed using cutting-edge models like OpenAI's GPT-4 or Anthropic's Claude 3.5 Sonnet.

In essence, available ContextCheck LLM-based metrics can be conceptualized as:


* **Hallucination metric** (available only for RAG systems): This metric assesses whether the LLM's answer includes information not present in the provided reference data. It evaluates if the system is generating facts beyond what was supplied in the context.

* **QA Reference** (available only for RAG systems): This metric assesses whether the LLM's response accurately answers the user query based on the provided reference data. Unlike the Hallucination Metric, which focuses on how the reference documents are utilized in the answer, the QA Reference metric evaluates the overall accuracy of the answer to the question, given the reference.

* **Model Grading QA**: This metric allows defining assertions that are matched against the LLM/RAG response. Think of it as "regular expressions defined using natural language." For example, given the query "Who are you?", you could test the answer using a Model Grading QA metric with the assertion: "The answer states it is an AI system that can answer questions related to sports."

* **Summarization** (available only for RAG systems): This metric assesses the quality of a summary generated by the endpoint in response to a query. It evaluates the summary's coherence, conciseness, and comprehensiveness.

* **Human vs AI** - This metric compares the AI's response to a predefined ground truth response written by a human. Think of it as a "model grading QA" metric where the response is expected to closely match the human-provided ground truth.


## Implementing metrics in the test scenarios

The implementation is similar to the deterministic metrics. 
Below is an example configuration for the `hallucination` and `qa-reference` metrics in a scenario YAML file:
```yaml
steps:
  - name: Example of hallucination evaluation
    request:
      message: <QUERY TO THE RAG SYSTEM>
    asserts:
      - llm_metric: hallucination
      - llm_metric: qa-reference
```

Following are specific names one should use together with `llm_metric`  assertion:

* `hallucination`

* `qa-reference`

* `model-grading-qa`

* `summarization`

* `human-vs-ai`

Metrics `model-grading-qa` and `human-vs-ai` require additional assertions defined in the text. Refer to details below. 

## Additional considerations

### Hallucination

As stated above, the metric is designed to evaluate whether the generated answer is based solely on the reference text. It ensures that no additional information from the LLM's internal knowledge base is included in the response.

Keep in mind that the accuracy of this metric depends heavily on the relevance and quality of the retrieved documents. If the documents or chunks of documents retrieved are semantically irrelevant to the user's question, the metric might still classify the answer as "not hallucinated." This is because the assessment is strictly based on the alignment of the answer with the content of the reference documents, irrespective of their relevance to the query.

### Model Grading QA examples and syntax

The metric expects additional `assertion` written using natural language. Examples:

```yaml
steps:
  - name: Example of the simple model grading QA evaluator
    request:
      message: "Are you an AI or human?"
    asserts:
      - llm_metric: model-grading-qa
        assertion: Should say it is a human.
  - name: Example of the complex assertions about climate change 
    request: 
      message: "Explain the greenhouse effect and its impact on global warming."
      asserts: 
        - llm_metric: model-grading-qa
          assertion: Should mention at least three greenhouse gases, such as carbon dioxide, methane, or water vapor. 
        - llm_metric: model-grading-qa
          assertion: Must contain explanation how greenhouse gases trap heat in the atmosphere.
        - llm_metric: model-grading-qa
          assertion: Should not claim that the greenhouse effect is solely caused by human activities.
        - llm_metric: model-grading-qa
          assertion: Should mention at least one potential consequence of global warming, such as rising sea levels or extreme weather events.
```

Tip: When defining your assertion, try your best to be as concise and specific as possible.

### Human vs AI example and syntax

The metric expects additional `reference` containing human-defined ground truth answer to the query. Example:

```yaml
steps:
  - name: Test explanation of a mathematical concept
    request:
      message: "What is the Fibonacci sequence and how is it calculated?"
    asserts:
      - llm_metric: human-vs-ai
        reference: "The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1. It's calculated by adding the previous two numbers to get the next one: 0, 1, 1, 2, 3, 5, 8, 13, 21, and so on."
```
