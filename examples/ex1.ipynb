{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "In this notebook we will present you a simple case of using contextcheck to validate llm responses.\n",
    "\n",
    "We will talk about:\n",
    "- Configuration\n",
    "- Test Scenario\n",
    "- Running the Test Scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create another test scenario where each type of assertion would be included\n",
    "# TODO: For this, config should also be updated\n",
    "# TODO: Talk about sub components separately (step1, step2, assertion1, assertion2 etc.)\n",
    "# TODO: Add optional jinja2 templating section or a remark with a link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install contextcheck\n",
    "# %pip install devtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextcheck import TestScenario\n",
    "from contextcheck.executors.executor import Executor # NOTE RB: Maybe Executor should be at the most outer layer for import\n",
    "from devtools import pprint # Needed for pydantic models pretty formatting\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario creation\n",
    "\n",
    "Note that throughout this notebook we present a separate bits of a single scenario which are all gathered in a proper yaml, which is used after the explanation of the particular parts which make a scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain config\n",
    "\n",
    "Config defines llm (or Rag system) connection. We provide several popular llm providers which lets you be productive from the start.\n",
    "There are three components used in config:\n",
    "1. `endpoint_under_test` - defines the tested endpoint\n",
    "2. `default_request` - defines the defaults for both the `endpoint_under_test` and `eval_endpoint` (TODO: Please someone confirm that)\n",
    "3. `eval_endpoint` - defines the endpoint which is used for evaluating the responses from `endpoint_under_test`\n",
    "\n",
    "For more infromation about configuration please go to [TODO - INSERT LINK HERE]\n",
    "\n",
    "TODO: What's the purpose of `default_request` when the same configuration can be given to `endpoint_under_test` or `eval_endpoint`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'endpoint_under_test': {'kind': 'openai',\n",
       "   'model': 'gpt-4o-mini',\n",
       "   'temperature': 0.2},\n",
       "  'eval_endpoint': {'kind': 'openai', 'model': 'gpt-4o', 'temperature': 0.0}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define configuration in yaml - for demonstration purposes it's done in notebook\n",
    "# TIP: For eval_endpoint try to use SOTA llm if possible\n",
    "yaml_config_1 = \"\"\"\n",
    "config:\n",
    "   endpoint_under_test:\n",
    "      kind: openai\n",
    "      model: gpt-4o-mini\n",
    "      temperature: 0.2\n",
    "   eval_endpoint:\n",
    "      kind: openai\n",
    "      model: gpt-4o\n",
    "      temperature: 0.0\n",
    "\"\"\"\n",
    "\n",
    "yaml_from_string = yaml.safe_load(yaml_config_1)\n",
    "yaml_from_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: Adding custom endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic or a link for creating and using custom endpoint should be added somewhere here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain steps\n",
    "\n",
    "Each test scenario consists of at least one testing step.\n",
    "\n",
    "Each step can by defined by its `name` (optional), `request` and `asserts` (optional):\n",
    "- `name` is a name of the test step\n",
    "- `request` is a message to an llm\n",
    "- `asserts` is a list of assertions done on llm response\n",
    "\n",
    "NOTE: By default each assert is treated as an `eval` assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [{'name': 'Check capital of Poland',\n",
       "   'request': 'What is the capital city of Poland?',\n",
       "   'asserts': ['\"Warsaw\" in response.message',\n",
       "    'response.stats.conn_duration < 3']},\n",
       "  {'name': 'Test hallucination evaluator (hallucinated)',\n",
       "   'request': {'message': 'Where did Mike go? Choose between the home and the park.'},\n",
       "   'asserts': [{'llm_metric': 'hallucination',\n",
       "     'reference': 'Mike went to the store.'}]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Add other type of asserts\n",
    "yaml_from_string = yaml.safe_load(\"\"\"\n",
    "steps:\n",
    "   - name: Check capital of Poland\n",
    "     request: 'What is the capital city of Poland?'\n",
    "     asserts:\n",
    "        - '\"Warsaw\" in response.message'\n",
    "        - 'response.stats.conn_duration < 3'\n",
    "   - name: Test hallucination evaluator (hallucinated)\n",
    "     request:\n",
    "       message: Where did Mike go? Choose between the home and the park.\n",
    "     asserts:\n",
    "        - llm_metric: hallucination\n",
    "          reference: Mike went to the store.\n",
    "\"\"\")\n",
    "yaml_from_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain assertions\n",
    "\n",
    "There are three families of assertions:\n",
    "1. `eval` assertion - converts a string to python code using (you guessed it) eval\n",
    "2. `llm_metric` assertion - uses another llm defined in `eval_endpoint` to assess the `endpoint_under_test` performance\n",
    "3. `deterministic` assertion - does string assessments like contains, contains-any etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add deterministic assertion combining it with the two previous assertions\n",
    "# NOTE RB: Metrics should be easilly extended i.e. if someone wants to add a metric we should provide a simple way\n",
    "# to do that, which should not break any functionalities like result summarization or time statistics etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain llm assertions\n",
    "\n",
    "`llm_metric` uses another llm to assess the response of the `endpoint_under_test`. For this `eval_endpoint` should be added in config section to define evaluation endpoint. It can be one of the available endpoints (link here) or one created by the user (link here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add 1-2 examples here and link other options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain eval assertions\n",
    "\n",
    "`eval` assertion uses python's build in eval function which changes any string to python executable code. User has Response model for disposition which include in a base form should include the response from the `endpoint_under_test` and the time statistics (see `ConnectorStats` model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add 1-2 examples of eval here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain deterministic assertions\n",
    "\n",
    "`deterministic` assertion provide a way to assert the content of the response through string comparisons like contains or contains-any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show 1-2 examples of that and link to other options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the test scenario is finally ready we can load it\n",
    "# TODO: Extend scenario_example1.yaml\n",
    "test_scenario_file_path = \"../tests/scenario_example1.yaml\"\n",
    "test_scenario = TestScenario.from_yaml(file_path=test_scenario_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestScenario(\n",
      "    steps=[\n",
      "        TestStep(\n",
      "            name='Check capital of Poland',\n",
      "            request=RequestBase(\n",
      "                message='What is the capital city of Poland?',\n",
      "            ),\n",
      "            response=None,\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='\"Warsaw\" in response.message',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "            ],\n",
      "            result=None,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Test hallucination evaluator (hallucinated)',\n",
      "            request=RequestBase(\n",
      "                message='Where did Mike go? Choose between the home and the park.',\n",
      "            ),\n",
      "            response=None,\n",
      "            asserts=[\n",
      "                AssertionLLM(\n",
      "                    result=None,\n",
      "                    llm_metric='hallucination',\n",
      "                    reference='Mike went to the store.',\n",
      "                    assertion='',\n",
      "                ),\n",
      "            ],\n",
      "            result=None,\n",
      "        ),\n",
      "    ],\n",
      "    config=TestConfig(\n",
      "        endpoint_under_test=EndpointConfig(\n",
      "            kind='openai',\n",
      "            url='',\n",
      "            model='gpt-4o-mini',\n",
      "            additional_headers={},\n",
      "            provider=None,\n",
      "            temperature=0.2,\n",
      "            max_tokens=None,\n",
      "            top_k=3,\n",
      "            use_ranker=True,\n",
      "            collection_name='default',\n",
      "        ),\n",
      "        default_request=None,\n",
      "        eval_endpoint=EndpointConfig(\n",
      "            kind='openai',\n",
      "            url='',\n",
      "            model='gpt-4o',\n",
      "            additional_headers={},\n",
      "            provider=None,\n",
      "            temperature=0.0,\n",
      "            max_tokens=None,\n",
      "            top_k=3,\n",
      "            use_ranker=True,\n",
      "            collection_name='default',\n",
      "        ),\n",
      "    ),\n",
      "    result=None,\n",
      "    filename='scenario_example1.yaml',\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of test_scenario\n",
    "pprint(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate executor which runs test scenario\n",
    "executor = Executor(test_scenario=test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-24 10:19:49.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[32m2024-09-24 10:19:49.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Check capital of Poland' request=RequestBase(message='What is the capital city of Poland?') response=None asserts=[AssertionEval(result=None, eval='\"Warsaw\" in response.message'), AssertionEval(result=None, eval='response.stats.conn_duration < 3')] result=None\u001b[0m\n",
      "\u001b[32m2024-09-24 10:19:49.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='What is the capital city of Poland?'\u001b[0m\n",
      "\u001b[32m2024-09-24 10:19:50.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='The capital city of Poland is Warsaw.' stats=ResponseStats(tokens_request=15, tokens_response=8, tokens_total=23, conn_start_time=7808.719369254, conn_end_time=7809.255086824, conn_duration=0.5357175699991785) id='chatcmpl-AAv3t0s4vtdA72ZXv6GNn9cWHsTYM' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'The capital city of Poland is Warsaw.', 'role': 'assistant', 'refusal': None}}] created=1727165989 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_1bb46167f9' usage={'completion_tokens': 8, 'prompt_tokens': 15, 'total_tokens': 23, 'completion_tokens_details': {'reasoning_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=0.2, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-09-24 10:19:50.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='\"Warsaw\" in response.message'\u001b[0m\n",
      "\u001b[32m2024-09-24 10:19:50.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='response.stats.conn_duration < 3'\u001b[0m\n",
      "\u001b[32m2024-09-24 10:19:50.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Test hallucination evaluator (hallucinated)' request=RequestBase(message='Where did Mike go? Choose between the home and the park.') response=None asserts=[AssertionLLM(result=None, llm_metric='hallucination', reference='Mike went to the store.', assertion='')] result=None\u001b[0m\n",
      "\u001b[32m2024-09-24 10:19:50.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Where did Mike go? Choose between the home and the park.'\u001b[0m\n",
      "\u001b[32m2024-09-24 10:19:50.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='I would need more context to determine where Mike went. If you have any additional details about his plans or activities, that could help!' stats=ResponseStats(tokens_request=20, tokens_response=27, tokens_total=47, conn_start_time=7809.259526912, conn_end_time=7809.911498477, conn_duration=0.6519715650001672) id='chatcmpl-AAv3uzWOjEFGlyazmG2XojxlIsDr6' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'I would need more context to determine where Mike went. If you have any additional details about his plans or activities, that could help!', 'role': 'assistant', 'refusal': None}}] created=1727165990 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_1bb46167f9' usage={'completion_tokens': 27, 'prompt_tokens': 20, 'total_tokens': 47, 'completion_tokens_details': {'reasoning_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=0.2, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-09-24 10:19:51.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=False llm_metric='hallucination' reference='Mike went to the store.' assertion='' metric_evaluator=LLMMetricEvaluator(eval_endpoint=EndpointOpenAI(connector=ConnectorOpenAI(stats=ConnectorStats(conn_start_time=7809.912424328, conn_end_time=7810.2899517, conn_duration=0.37752737199934927), model='gpt-4o'), config=EndpointConfig(kind='openai', url='', model='gpt-4o', additional_headers={}, provider=None, temperature=0.0, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')), metric=MetricHallucination(prompt_template='\\nIn this task, you will be presented with a query, a reference text and an answer. The answer is\\ngenerated to the question based on the reference text. The answer may contain false information. You\\nmust use the reference text to determine if the answer to the question contains false information,\\nif the answer is a hallucination of facts. Your objective is to determine whether the answer text\\ncontains factual information and is not a hallucination. A \\'hallucination\\' refers to\\nan answer that is not based on the reference text or assumes information that is not available in\\nthe reference text. \"hallucinated\" indicates that the answer\\nprovides factually inaccurate information to the query based on the reference text. \"factual\"\\nindicates that the answer to the question is correct relative to the reference text, and does not\\ncontain made up information. Please read the query and reference text carefully before determining\\nyour response.\\n\\n[BEGIN DATA]\\n************\\n[Query]: {input}\\n************\\n[Reference text]: {reference}\\n************\\n[Answer]: {output}\\n************\\n[END DATA]\\n\\nIs the answer above factual or hallucinated based on the query and reference text? \\nYour response should be a single word: either \"factual\" or \"hallucinated\", and it should not include any other text or characters.\\n', rails={'factual': True, 'hallucinated': False}))\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run test scenario\n",
    "executor.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestScenario(\n",
      "    steps=[\n",
      "        TestStep(\n",
      "            name='Check capital of Poland',\n",
      "            request=RequestBase(\n",
      "                message='What is the capital city of Poland?',\n",
      "            ),\n",
      "            response=ResponseModel(\n",
      "                message='The capital city of Poland is Warsaw.',\n",
      "                stats=ResponseStats(\n",
      "                    tokens_request=15,\n",
      "                    tokens_response=8,\n",
      "                    tokens_total=23,\n",
      "                    conn_start_time=7808.719369254,\n",
      "                    conn_end_time=7809.255086824,\n",
      "                    conn_duration=0.5357175699991785,\n",
      "                ),\n",
      "                id='chatcmpl-AAv3t0s4vtdA72ZXv6GNn9cWHsTYM',\n",
      "                choices=[\n",
      "                    {\n",
      "                        'finish_reason': 'stop',\n",
      "                        'index': 0,\n",
      "                        'logprobs': None,\n",
      "                        'message': {\n",
      "                            'content': 'The capital city of Poland is Warsaw.',\n",
      "                            'role': 'assistant',\n",
      "                            'refusal': None,\n",
      "                        },\n",
      "                    },\n",
      "                ],\n",
      "                created=1727165989,\n",
      "                model='gpt-4o-mini-2024-07-18',\n",
      "                object='chat.completion',\n",
      "                system_fingerprint='fp_1bb46167f9',\n",
      "                usage={\n",
      "                    'completion_tokens': 8,\n",
      "                    'prompt_tokens': 15,\n",
      "                    'total_tokens': 23,\n",
      "                    'completion_tokens_details': {\n",
      "                        'reasoning_tokens': 0,\n",
      "                    },\n",
      "                },\n",
      "                config=EndpointConfig(\n",
      "                    kind='openai',\n",
      "                    url='',\n",
      "                    model='gpt-4o-mini',\n",
      "                    additional_headers={},\n",
      "                    provider=None,\n",
      "                    temperature=0.2,\n",
      "                    max_tokens=None,\n",
      "                    top_k=3,\n",
      "                    use_ranker=True,\n",
      "                    collection_name='default',\n",
      "                ),\n",
      "            ),\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='\"Warsaw\" in response.message',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "            ],\n",
      "            result=True,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Test hallucination evaluator (hallucinated)',\n",
      "            request=RequestBase(\n",
      "                message='Where did Mike go? Choose between the home and the park.',\n",
      "            ),\n",
      "            response=ResponseModel(\n",
      "                message=(\n",
      "                    'I would need more context to determine where Mike went. If you have any additional details about '\n",
      "                    'his plans or activities, that could help!'\n",
      "                ),\n",
      "                stats=ResponseStats(\n",
      "                    tokens_request=20,\n",
      "                    tokens_response=27,\n",
      "                    tokens_total=47,\n",
      "                    conn_start_time=7809.259526912,\n",
      "                    conn_end_time=7809.911498477,\n",
      "                    conn_duration=0.6519715650001672,\n",
      "                ),\n",
      "                id='chatcmpl-AAv3uzWOjEFGlyazmG2XojxlIsDr6',\n",
      "                choices=[\n",
      "                    {\n",
      "                        'finish_reason': 'stop',\n",
      "                        'index': 0,\n",
      "                        'logprobs': None,\n",
      "                        'message': {\n",
      "                            'content': (\n",
      "                                'I would need more context to determine where Mike went. If you have any additional de'\n",
      "                                'tails about his plans or activities, that could help!'\n",
      "                            ),\n",
      "                            'role': 'assistant',\n",
      "                            'refusal': None,\n",
      "                        },\n",
      "                    },\n",
      "                ],\n",
      "                created=1727165990,\n",
      "                model='gpt-4o-mini-2024-07-18',\n",
      "                object='chat.completion',\n",
      "                system_fingerprint='fp_1bb46167f9',\n",
      "                usage={\n",
      "                    'completion_tokens': 27,\n",
      "                    'prompt_tokens': 20,\n",
      "                    'total_tokens': 47,\n",
      "                    'completion_tokens_details': {\n",
      "                        'reasoning_tokens': 0,\n",
      "                    },\n",
      "                },\n",
      "                config=EndpointConfig(\n",
      "                    kind='openai',\n",
      "                    url='',\n",
      "                    model='gpt-4o-mini',\n",
      "                    additional_headers={},\n",
      "                    provider=None,\n",
      "                    temperature=0.2,\n",
      "                    max_tokens=None,\n",
      "                    top_k=3,\n",
      "                    use_ranker=True,\n",
      "                    collection_name='default',\n",
      "                ),\n",
      "            ),\n",
      "            asserts=[\n",
      "                AssertionLLM(\n",
      "                    result=False,\n",
      "                    llm_metric='hallucination',\n",
      "                    reference='Mike went to the store.',\n",
      "                    assertion='',\n",
      "                    metric_evaluator=LLMMetricEvaluator(\n",
      "                        eval_endpoint=EndpointOpenAI(\n",
      "                            connector=ConnectorOpenAI(\n",
      "                                stats=ConnectorStats(\n",
      "                                    conn_start_time=7809.912424328,\n",
      "                                    conn_end_time=7810.2899517,\n",
      "                                    conn_duration=0.37752737199934927,\n",
      "                                ),\n",
      "                                model='gpt-4o',\n",
      "                            ),\n",
      "                            config=EndpointConfig(\n",
      "                                kind='openai',\n",
      "                                url='',\n",
      "                                model='gpt-4o',\n",
      "                                additional_headers={},\n",
      "                                provider=None,\n",
      "                                temperature=0.0,\n",
      "                                max_tokens=None,\n",
      "                                top_k=3,\n",
      "                                use_ranker=True,\n",
      "                                collection_name='default',\n",
      "                            ),\n",
      "                        ),\n",
      "                        metric=MetricHallucination(\n",
      "                            prompt_template=(\n",
      "                                '\\n'\n",
      "                                'In this task, you will be presented with a query, a reference text and an answer. The'\n",
      "                                ' answer is\\n'\n",
      "                                'generated to the question based on the reference text. The answer may contain false i'\n",
      "                                'nformation. You\\n'\n",
      "                                'must use the reference text to determine if the answer to the question contains false'\n",
      "                                ' information,\\n'\n",
      "                                'if the answer is a hallucination of facts. Your objective is to determine whether the'\n",
      "                                ' answer text\\n'\n",
      "                                \"contains factual information and is not a hallucination. A 'hallucination' refers to\\n\"\n",
      "                                'an answer that is not based on the reference text or assumes information that is not '\n",
      "                                'available in\\n'\n",
      "                                'the reference text. \"hallucinated\" indicates that the answer\\n'\n",
      "                                'provides factually inaccurate information to the query based on the reference text. \"'\n",
      "                                'factual\"\\n'\n",
      "                                'indicates that the answer to the question is correct relative to the reference text, '\n",
      "                                'and does not\\n'\n",
      "                                'contain made up information. Please read the query and reference text carefully befor'\n",
      "                                'e determining\\n'\n",
      "                                'your response.\\n'\n",
      "                                '\\n'\n",
      "                                '[BEGIN DATA]\\n'\n",
      "                                '************\\n'\n",
      "                                '[Query]: {input}\\n'\n",
      "                                '************\\n'\n",
      "                                '[Reference text]: {reference}\\n'\n",
      "                                '************\\n'\n",
      "                                '[Answer]: {output}\\n'\n",
      "                                '************\\n'\n",
      "                                '[END DATA]\\n'\n",
      "                                '\\n'\n",
      "                                'Is the answer above factual or hallucinated based on the query and reference text? \\n'\n",
      "                                'Your response should be a single word: either \"factual\" or \"hallucinated\", and it sho'\n",
      "                                'uld not include any other text or characters.\\n'\n",
      "                            ),\n",
      "                            rails={\n",
      "                                'factual': True,\n",
      "                                'hallucinated': False,\n",
      "                            },\n",
      "                        ),\n",
      "                    ),\n",
      "                ),\n",
      "            ],\n",
      "            result=False,\n",
      "        ),\n",
      "    ],\n",
      "    config=TestConfig(\n",
      "        endpoint_under_test=EndpointConfig(\n",
      "            kind='openai',\n",
      "            url='',\n",
      "            model='gpt-4o-mini',\n",
      "            additional_headers={},\n",
      "            provider=None,\n",
      "            temperature=0.2,\n",
      "            max_tokens=None,\n",
      "            top_k=3,\n",
      "            use_ranker=True,\n",
      "            collection_name='default',\n",
      "        ),\n",
      "        default_request=None,\n",
      "        eval_endpoint=EndpointConfig(\n",
      "            kind='openai',\n",
      "            url='',\n",
      "            model='gpt-4o',\n",
      "            additional_headers={},\n",
      "            provider=None,\n",
      "            temperature=0.0,\n",
      "            max_tokens=None,\n",
      "            top_k=3,\n",
      "            use_ranker=True,\n",
      "            collection_name='default',\n",
      "        ),\n",
      "    ),\n",
      "    result=False,\n",
      "    filename='scenario_example1.yaml',\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# NOTE RB: Maybe executor should copy the test scenario\n",
    "# Inspect updated test_scenario\n",
    "pprint(test_scenario) # It shows api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step name: Check capital of Poland, Result: True\n",
      "Step name: Test hallucination evaluator (hallucinated), Result: False\n"
     ]
    }
   ],
   "source": [
    "# We can inspect each test step separately and check its results\n",
    "for step in test_scenario.steps:\n",
    "    print(f\"Step name: {step.name}, Result: {step.result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step name: Check capital of Poland:\n",
      "\n",
      "result=True eval='\"Warsaw\" in response.message'\n",
      "result=True eval='response.stats.conn_duration < 3'\n",
      "------------\n",
      "Step name: Test hallucination evaluator (hallucinated):\n",
      "\n",
      "result=False llm_metric='hallucination' reference='Mike went to the store.' assertion='' metric_evaluator=LLMMetricEvaluator(eval_endpoint=EndpointOpenAI(connector=ConnectorOpenAI(stats=ConnectorStats(conn_start_time=7809.912424328, conn_end_time=7810.2899517, conn_duration=0.37752737199934927), model='gpt-4o'), config=EndpointConfig(kind='openai', url='', model='gpt-4o', additional_headers={}, provider=None, temperature=0.0, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')), metric=MetricHallucination(prompt_template='\\nIn this task, you will be presented with a query, a reference text and an answer. The answer is\\ngenerated to the question based on the reference text. The answer may contain false information. You\\nmust use the reference text to determine if the answer to the question contains false information,\\nif the answer is a hallucination of facts. Your objective is to determine whether the answer text\\ncontains factual information and is not a hallucination. A \\'hallucination\\' refers to\\nan answer that is not based on the reference text or assumes information that is not available in\\nthe reference text. \"hallucinated\" indicates that the answer\\nprovides factually inaccurate information to the query based on the reference text. \"factual\"\\nindicates that the answer to the question is correct relative to the reference text, and does not\\ncontain made up information. Please read the query and reference text carefully before determining\\nyour response.\\n\\n[BEGIN DATA]\\n************\\n[Query]: {input}\\n************\\n[Reference text]: {reference}\\n************\\n[Answer]: {output}\\n************\\n[END DATA]\\n\\nIs the answer above factual or hallucinated based on the query and reference text? \\nYour response should be a single word: either \"factual\" or \"hallucinated\", and it should not include any other text or characters.\\n', rails={'factual': True, 'hallucinated': False}))\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# # We can also inspect each assertion for each step separately\n",
    "for step in test_scenario.steps:\n",
    "    print(f\"Step name: {step.name}:\\n\")\n",
    "    for assertion in step.asserts:\n",
    "        print(assertion)\n",
    "    print(\"-\"*12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute scenario using ccheck comments - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
