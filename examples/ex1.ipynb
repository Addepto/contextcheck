{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "Most likely some comments here like - what will be presented, and the structure of the notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create another test scenario where each type of assertion would be included\n",
    "# TODO: For this, config should also be updated\n",
    "# TODO: Talk about sub components separately (step1, step2, assertion1, assertion2 etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install contextcheck\n",
    "# %pip install devtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextcheck import TestScenario\n",
    "from contextcheck.executors.executor import Executor # NOTE RB: Maybe Executor should be at the most outer layer for import\n",
    "from devtools import pprint # Needed for pydantic models pretty formatting\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'endpoint_under_test': {'kind': 'openai'}}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define configuration in yaml\n",
    "# endpoint_under_test entry defines the tested endpoint\n",
    "# The list of available kinds and models is available under: [here comes the link]\n",
    "yaml_from_string = yaml.safe_load(\"\"\"\n",
    "config:\n",
    "   endpoint_under_test:\n",
    "      kind: openai\n",
    "      # Optional model name\n",
    "      # model: gpt-4o-mini\n",
    "\"\"\")\n",
    "yaml_from_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': ['Write success in the response',\n",
       "  {'name': 'Check capital of Poland',\n",
       "   'request': 'What is the capital city of Poland?',\n",
       "   'asserts': ['\"Warsaw\" in response.message',\n",
       "    'response.stats.conn_duration < 3']}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each test scenario consists of at least on testing step\n",
    "# Each step can by defined by its name (optional), request and asserts (optional)\n",
    "# `request` is a message to an llm\n",
    "# `asserts` is a list of assertions done on llm response\n",
    "yaml_from_string = yaml.safe_load(\"\"\"\n",
    "steps:\n",
    "   - name: Check capital of Poland\n",
    "     request: 'What is the capital city of Poland?'\n",
    "     asserts:\n",
    "        - '\"Warsaw\" in response.message'\n",
    "        - 'response.stats.conn_duration < 3'\n",
    "\"\"\")\n",
    "yaml_from_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain step (general schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain llm assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM metrics use another llm to assess the response of another llm\n",
    "# for this eval_endpoint should be added in config section to define evaluation endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain eval assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval assertions use python's build in eval to evaluate the use made evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain deterministic assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the test scenario is finally ready we can load it\n",
    "test_scenario_file_path = \"../tests/scenario_openai.yaml\"\n",
    "test_scenario = TestScenario.from_yaml(file_path=test_scenario_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestScenario(\n",
      "    steps=[\n",
      "        TestStep(\n",
      "            name='Write success in the response',\n",
      "            request=RequestBase(\n",
      "                message='Write success in the response',\n",
      "            ),\n",
      "            response=None,\n",
      "            asserts=[],\n",
      "            result=None,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Check capital of Poland',\n",
      "            request=RequestBase(\n",
      "                message='What is the capital city of Poland?',\n",
      "            ),\n",
      "            response=None,\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='\"Warsaw\" in response.message',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "            ],\n",
      "            result=None,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Send hello',\n",
      "            request=RequestBase(\n",
      "                message='Hello!',\n",
      "            ),\n",
      "            response=None,\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='response.stats.tokens_response > 5',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='\"Goodbye\" in response.message',\n",
      "                ),\n",
      "            ],\n",
      "            result=None,\n",
      "        ),\n",
      "    ],\n",
      "    config=TestConfig(\n",
      "        endpoint_under_test=EndpointConfig(\n",
      "            kind='openai',\n",
      "            url='',\n",
      "            model='gpt-4o-mini',\n",
      "            additional_headers={},\n",
      "            provider=None,\n",
      "            temperature=None,\n",
      "            max_tokens=None,\n",
      "            top_k=3,\n",
      "            use_ranker=True,\n",
      "            collection_name='default',\n",
      "        ),\n",
      "        default_request=None,\n",
      "        eval_endpoint=None,\n",
      "    ),\n",
      "    result=None,\n",
      "    filename='scenario_openai.yaml',\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of test_scenario\n",
    "pprint(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate executor which runes test scenario\n",
    "executor = Executor(test_scenario=test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-16 14:05:03.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:03.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Write success in the response' request=RequestBase(message='Write success in the response') response=None asserts=[] result=None\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:03.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Write success in the response'\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:03.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Success! How can I assist you further?' stats=ResponseStats(tokens_request=12, tokens_response=9, tokens_total=21, conn_start_time=18873.231008109, conn_end_time=18873.839623668, conn_duration=0.6086155589982809) id='chatcmpl-A84lIXOMMNw5lANRJDlaJXCHgj9ZS' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Success! How can I assist you further?', 'role': 'assistant', 'refusal': None}}] created=1726488292 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_483d39d857' usage={'completion_tokens': 9, 'prompt_tokens': 12, 'total_tokens': 21, 'completion_tokens_details': {'reasoning_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=None, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:03.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Check capital of Poland' request=RequestBase(message='What is the capital city of Poland?') response=None asserts=[AssertionEval(result=None, eval='\"Warsaw\" in response.message'), AssertionEval(result=None, eval='response.stats.conn_duration < 3')] result=None\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:03.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='What is the capital city of Poland?'\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:04.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='The capital city of Poland is Warsaw.' stats=ResponseStats(tokens_request=15, tokens_response=8, tokens_total=23, conn_start_time=18873.84174941, conn_end_time=18874.425237093, conn_duration=0.5834876830012945) id='chatcmpl-A84lJojCSgNQIuY05jsQ3U1famdqc' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'The capital city of Poland is Warsaw.', 'role': 'assistant', 'refusal': None}}] created=1726488293 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_483d39d857' usage={'completion_tokens': 8, 'prompt_tokens': 15, 'total_tokens': 23, 'completion_tokens_details': {'reasoning_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=None, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:04.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='\"Warsaw\" in response.message'\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:04.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='response.stats.conn_duration < 3'\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:04.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Send hello' request=RequestBase(message='Hello!') response=None asserts=[AssertionEval(result=None, eval='response.stats.conn_duration < 3'), AssertionEval(result=None, eval='response.stats.tokens_response > 5'), AssertionEval(result=None, eval='\"Goodbye\" in response.message')] result=None\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:04.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Hello!'\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:04.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Hello! How can I assist you today?' stats=ResponseStats(tokens_request=9, tokens_response=9, tokens_total=18, conn_start_time=18874.429641249, conn_end_time=18874.883169424, conn_duration=0.45352817499951925) id='chatcmpl-A84lJZGkLqmB4DQBzFOC3kJPJZhB7' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Hello! How can I assist you today?', 'role': 'assistant', 'refusal': None}}] created=1726488293 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_483d39d857' usage={'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'reasoning_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=None, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:04.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='response.stats.conn_duration < 3'\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:04.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='response.stats.tokens_response > 5'\u001b[0m\n",
      "\u001b[32m2024-09-16 14:05:04.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=False eval='\"Goodbye\" in response.message'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run test scenario\n",
    "executor.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestScenario(\n",
      "    steps=[\n",
      "        TestStep(\n",
      "            name='Write success in the response',\n",
      "            request=RequestBase(\n",
      "                message='Write success in the response',\n",
      "            ),\n",
      "            response=ResponseModel(\n",
      "                message='Success! How can I assist you further?',\n",
      "                stats=ResponseStats(\n",
      "                    tokens_request=12,\n",
      "                    tokens_response=9,\n",
      "                    tokens_total=21,\n",
      "                    conn_start_time=18873.231008109,\n",
      "                    conn_end_time=18873.839623668,\n",
      "                    conn_duration=0.6086155589982809,\n",
      "                ),\n",
      "                id='chatcmpl-A84lIXOMMNw5lANRJDlaJXCHgj9ZS',\n",
      "                choices=[\n",
      "                    {\n",
      "                        'finish_reason': 'stop',\n",
      "                        'index': 0,\n",
      "                        'logprobs': None,\n",
      "                        'message': {\n",
      "                            'content': 'Success! How can I assist you further?',\n",
      "                            'role': 'assistant',\n",
      "                            'refusal': None,\n",
      "                        },\n",
      "                    },\n",
      "                ],\n",
      "                created=1726488292,\n",
      "                model='gpt-4o-mini-2024-07-18',\n",
      "                object='chat.completion',\n",
      "                system_fingerprint='fp_483d39d857',\n",
      "                usage={\n",
      "                    'completion_tokens': 9,\n",
      "                    'prompt_tokens': 12,\n",
      "                    'total_tokens': 21,\n",
      "                    'completion_tokens_details': {\n",
      "                        'reasoning_tokens': 0,\n",
      "                    },\n",
      "                },\n",
      "                config=EndpointConfig(\n",
      "                    kind='openai',\n",
      "                    url='',\n",
      "                    model='gpt-4o-mini',\n",
      "                    additional_headers={},\n",
      "                    provider=None,\n",
      "                    temperature=None,\n",
      "                    max_tokens=None,\n",
      "                    top_k=3,\n",
      "                    use_ranker=True,\n",
      "                    collection_name='default',\n",
      "                ),\n",
      "            ),\n",
      "            asserts=[],\n",
      "            result=True,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Check capital of Poland',\n",
      "            request=RequestBase(\n",
      "                message='What is the capital city of Poland?',\n",
      "            ),\n",
      "            response=ResponseModel(\n",
      "                message='The capital city of Poland is Warsaw.',\n",
      "                stats=ResponseStats(\n",
      "                    tokens_request=15,\n",
      "                    tokens_response=8,\n",
      "                    tokens_total=23,\n",
      "                    conn_start_time=18873.84174941,\n",
      "                    conn_end_time=18874.425237093,\n",
      "                    conn_duration=0.5834876830012945,\n",
      "                ),\n",
      "                id='chatcmpl-A84lJojCSgNQIuY05jsQ3U1famdqc',\n",
      "                choices=[\n",
      "                    {\n",
      "                        'finish_reason': 'stop',\n",
      "                        'index': 0,\n",
      "                        'logprobs': None,\n",
      "                        'message': {\n",
      "                            'content': 'The capital city of Poland is Warsaw.',\n",
      "                            'role': 'assistant',\n",
      "                            'refusal': None,\n",
      "                        },\n",
      "                    },\n",
      "                ],\n",
      "                created=1726488293,\n",
      "                model='gpt-4o-mini-2024-07-18',\n",
      "                object='chat.completion',\n",
      "                system_fingerprint='fp_483d39d857',\n",
      "                usage={\n",
      "                    'completion_tokens': 8,\n",
      "                    'prompt_tokens': 15,\n",
      "                    'total_tokens': 23,\n",
      "                    'completion_tokens_details': {\n",
      "                        'reasoning_tokens': 0,\n",
      "                    },\n",
      "                },\n",
      "                config=EndpointConfig(\n",
      "                    kind='openai',\n",
      "                    url='',\n",
      "                    model='gpt-4o-mini',\n",
      "                    additional_headers={},\n",
      "                    provider=None,\n",
      "                    temperature=None,\n",
      "                    max_tokens=None,\n",
      "                    top_k=3,\n",
      "                    use_ranker=True,\n",
      "                    collection_name='default',\n",
      "                ),\n",
      "            ),\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='\"Warsaw\" in response.message',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "            ],\n",
      "            result=True,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Send hello',\n",
      "            request=RequestBase(\n",
      "                message='Hello!',\n",
      "            ),\n",
      "            response=ResponseModel(\n",
      "                message='Hello! How can I assist you today?',\n",
      "                stats=ResponseStats(\n",
      "                    tokens_request=9,\n",
      "                    tokens_response=9,\n",
      "                    tokens_total=18,\n",
      "                    conn_start_time=18874.429641249,\n",
      "                    conn_end_time=18874.883169424,\n",
      "                    conn_duration=0.45352817499951925,\n",
      "                ),\n",
      "                id='chatcmpl-A84lJZGkLqmB4DQBzFOC3kJPJZhB7',\n",
      "                choices=[\n",
      "                    {\n",
      "                        'finish_reason': 'stop',\n",
      "                        'index': 0,\n",
      "                        'logprobs': None,\n",
      "                        'message': {\n",
      "                            'content': 'Hello! How can I assist you today?',\n",
      "                            'role': 'assistant',\n",
      "                            'refusal': None,\n",
      "                        },\n",
      "                    },\n",
      "                ],\n",
      "                created=1726488293,\n",
      "                model='gpt-4o-mini-2024-07-18',\n",
      "                object='chat.completion',\n",
      "                system_fingerprint='fp_483d39d857',\n",
      "                usage={\n",
      "                    'completion_tokens': 9,\n",
      "                    'prompt_tokens': 9,\n",
      "                    'total_tokens': 18,\n",
      "                    'completion_tokens_details': {\n",
      "                        'reasoning_tokens': 0,\n",
      "                    },\n",
      "                },\n",
      "                config=EndpointConfig(\n",
      "                    kind='openai',\n",
      "                    url='',\n",
      "                    model='gpt-4o-mini',\n",
      "                    additional_headers={},\n",
      "                    provider=None,\n",
      "                    temperature=None,\n",
      "                    max_tokens=None,\n",
      "                    top_k=3,\n",
      "                    use_ranker=True,\n",
      "                    collection_name='default',\n",
      "                ),\n",
      "            ),\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='response.stats.tokens_response > 5',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=False,\n",
      "                    eval='\"Goodbye\" in response.message',\n",
      "                ),\n",
      "            ],\n",
      "            result=False,\n",
      "        ),\n",
      "    ],\n",
      "    config=TestConfig(\n",
      "        endpoint_under_test=EndpointConfig(\n",
      "            kind='openai',\n",
      "            url='',\n",
      "            model='gpt-4o-mini',\n",
      "            additional_headers={},\n",
      "            provider=None,\n",
      "            temperature=None,\n",
      "            max_tokens=None,\n",
      "            top_k=3,\n",
      "            use_ranker=True,\n",
      "            collection_name='default',\n",
      "        ),\n",
      "        default_request=None,\n",
      "        eval_endpoint=None,\n",
      "    ),\n",
      "    result=False,\n",
      "    filename='scenario_openai.yaml',\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# NOTE RB: Maybe executor should copy the test scenario\n",
    "# Inspect updated test_scenario\n",
    "pprint(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step name: Write success in the response, Result: True\n",
      "Step name: Check capital of Poland, Result: True\n",
      "Step name: Send hello, Result: False\n"
     ]
    }
   ],
   "source": [
    "# We can inspect each test step separately and check its results\n",
    "for step in test_scenario.steps:\n",
    "    print(f\"Step name: {step.name}, Result: {step.result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
