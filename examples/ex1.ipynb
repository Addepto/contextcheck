{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "Most likely some comments here like - what will be presented, and the structure of the notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create another test scenario where each type of assertion would be included\n",
    "# TODO: For this, config should also be updated\n",
    "# TODO: Talk about sub components separately (step1, step2, assertion1, assertion2 etc.)\n",
    "# TODO: Add optional jinja2 templating section or a remark with a link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install contextcheck\n",
    "# %pip install devtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextcheck import TestScenario\n",
    "from contextcheck.executors.executor import Executor # NOTE RB: Maybe Executor should be at the most outer layer for import\n",
    "from devtools import pprint # Needed for pydantic models pretty formatting\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario creation\n",
    "\n",
    "Note that throughout this notebook we present a separate bits of a single scenario which are all gathered in a proper yaml, which is used after the explanation of the particular parts which make a scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain config\n",
    "\n",
    "Config defines llm (or Rag system) connection. We provide several popular llm providers which lets you be productive from the start.\n",
    "There are three components used in config:\n",
    "1. `endpoint_under_test` - defines the tested endpoint\n",
    "2. `default_request` - defines the defaults for both the `endpoint_under_test` and `eval_endpoint` (TODO: Please someone confirm that)\n",
    "3. `eval_endpoint` - defines the endpoint which is used for evaluating the responses from `endpoint_under_test`\n",
    "\n",
    "For more infromation about configuration please go to [TODO - INSERT LINK HERE]\n",
    "\n",
    "TODO: What's the purpose of `default_request` when the same configuration can be given to `endpoint_under_test` or `eval_endpoint`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'endpoint_under_test': {'kind': 'openai', 'model': 'gpt-4o-mini'},\n",
       "  'eval_endpoint': {'kind': 'openai', 'model': 'gpt-4o-mini'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define configuration in yaml - for demonstration purposes it's done in notebook\n",
    "yaml_config_1 = \"\"\"\n",
    "config:\n",
    "   endpoint_under_test:\n",
    "      kind: openai\n",
    "      model: gpt-4o-mini\n",
    "      temperature: 0.2\n",
    "   eval_endpoint:\n",
    "      kind: openai\n",
    "      model: gpt-4o\n",
    "      temperature: 0.0\n",
    "\"\"\"\n",
    "\n",
    "yaml_from_string = yaml.safe_load(yaml_config_1)\n",
    "yaml_from_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: Adding custom endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic or a link for creating and using custom endpoint should be added somewhere here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain steps\n",
    "\n",
    "Each test scenario consists of at least one testing step.\n",
    "\n",
    "Each step can by defined by its `name` (optional), `request` and `asserts` (optional):\n",
    "- `name` is a name of the test step\n",
    "- `request` is a message to an llm\n",
    "- `asserts` is a list of assertions done on llm response\n",
    "\n",
    "NOTE: By default each assert is treated as an `eval` assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [{'name': 'Check capital of Poland',\n",
       "   'request': 'What is the capital city of Poland?',\n",
       "   'asserts': ['\"Warsaw\" in response.message',\n",
       "    'response.stats.conn_duration < 3']},\n",
       "  {'name': 'Test hallucination evaluator (hallucinated)',\n",
       "   'request': {'message': 'Where did Mike go? Choose between the home and the park.'},\n",
       "   'asserts': [{'llm_metric': 'hallucination',\n",
       "     'reference': 'Mike went to the store.'}]}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Add other type of asserts\n",
    "yaml_from_string = yaml.safe_load(\"\"\"\n",
    "steps:\n",
    "   - name: Check capital of Poland\n",
    "     request: 'What is the capital city of Poland?'\n",
    "     asserts:\n",
    "        - '\"Warsaw\" in response.message'\n",
    "        - 'response.stats.conn_duration < 3'\n",
    "   - name: Test hallucination evaluator (hallucinated)\n",
    "     request:\n",
    "       message: Where did Mike go? Choose between the home and the park.\n",
    "     asserts:\n",
    "        - llm_metric: hallucination\n",
    "          reference: Mike went to the store.\n",
    "\"\"\")\n",
    "yaml_from_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain assertions\n",
    "\n",
    "There are three families of assertions:\n",
    "1. `eval` assertion - converts a string to python code using (you guessed it) eval\n",
    "2. `llm_metric` assertion - uses another llm defined in `eval_endpoint` to assess the `endpoint_under_test` performance\n",
    "3. `deterministic` assertion - does string assessments like contains, contains-any etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add deterministic assertion combining it with the two previous assertions\n",
    "# NOTE RB: Metrics should be easilly extended i.e. if someone wants to add a metric we should provide a simple way\n",
    "# to do that, which should not break any functionalities like result summarization or time statistics etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain llm assertions\n",
    "\n",
    "`llm_metric` uses another llm to assess the response of the `endpoint_under_test`. For this `eval_endpoint` should be added in config section to define evaluation endpoint. It can be one of the available endpoints (link here) or one created by the user (link here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add 1-2 examples here and link other options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain eval assertions\n",
    "\n",
    "`eval` assertion uses python's build in eval function which changes any string to python executable code. User has Response model for disposition which include in a base form should include the response from the `endpoint_under_test` and the time statistics (see `ConnectorStats` model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add 1-2 examples of eval here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain deterministic assertions\n",
    "\n",
    "`deterministic` assertion provide a way to assert the content of the response through string comparisons like contains or contains-any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Show 1-2 examples of that and link to other options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the test scenario is finally ready we can load it\n",
    "# TODO: Change it to a proper test scenario\n",
    "test_scenario_file_path = \"../tests/scenario_openai.yaml\"\n",
    "test_scenario = TestScenario.from_yaml(file_path=test_scenario_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestScenario(\n",
      "    steps=[\n",
      "        TestStep(\n",
      "            name='Write success in the response',\n",
      "            request=RequestBase(\n",
      "                message='Write success in the response',\n",
      "            ),\n",
      "            response=None,\n",
      "            asserts=[],\n",
      "            result=None,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Check capital of Poland',\n",
      "            request=RequestBase(\n",
      "                message='What is the capital city of Poland?',\n",
      "            ),\n",
      "            response=None,\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='\"Warsaw\" in response.message',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "            ],\n",
      "            result=None,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Send hello',\n",
      "            request=RequestBase(\n",
      "                message='Hello!',\n",
      "            ),\n",
      "            response=None,\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='response.stats.tokens_response > 5',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=None,\n",
      "                    eval='\"Goodbye\" in response.message',\n",
      "                ),\n",
      "            ],\n",
      "            result=None,\n",
      "        ),\n",
      "    ],\n",
      "    config=TestConfig(\n",
      "        endpoint_under_test=EndpointConfig(\n",
      "            kind='openai',\n",
      "            url='',\n",
      "            model='gpt-4o-mini',\n",
      "            additional_headers={},\n",
      "            provider=None,\n",
      "            temperature=None,\n",
      "            max_tokens=None,\n",
      "            top_k=3,\n",
      "            use_ranker=True,\n",
      "            collection_name='default',\n",
      "        ),\n",
      "        default_request=None,\n",
      "        eval_endpoint=None,\n",
      "    ),\n",
      "    result=None,\n",
      "    filename='scenario_openai.yaml',\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of test_scenario\n",
    "pprint(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate executor which runs test scenario\n",
    "executor = Executor(test_scenario=test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-23 17:47:51.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:51.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Write success in the response' request=RequestBase(message='Write success in the response') response=None asserts=[] result=None\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:51.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Write success in the response'\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:52.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Success! How can I assist you further?' stats=ResponseStats(tokens_request=12, tokens_response=9, tokens_total=21, conn_start_time=26156.204792279, conn_end_time=26157.269648521, conn_duration=1.0648562419992231) id='chatcmpl-AAfZwElri6ZErMaaIxv12EXRAzQ1S' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Success! How can I assist you further?', 'role': 'assistant', 'refusal': None}}] created=1727106472 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_1bb46167f9' usage={'completion_tokens': 9, 'prompt_tokens': 12, 'total_tokens': 21, 'completion_tokens_details': {'reasoning_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=None, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:52.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Check capital of Poland' request=RequestBase(message='What is the capital city of Poland?') response=None asserts=[AssertionEval(result=None, eval='\"Warsaw\" in response.message'), AssertionEval(result=None, eval='response.stats.conn_duration < 3')] result=None\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:52.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='What is the capital city of Poland?'\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:53.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='The capital city of Poland is Warsaw.' stats=ResponseStats(tokens_request=15, tokens_response=8, tokens_total=23, conn_start_time=26157.274484821, conn_end_time=26157.799638476, conn_duration=0.5251536550022138) id='chatcmpl-AAfZxBVNsg5hvd9TQE2bc5hMnFZIk' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'The capital city of Poland is Warsaw.', 'role': 'assistant', 'refusal': None}}] created=1727106473 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_1bb46167f9' usage={'completion_tokens': 8, 'prompt_tokens': 15, 'total_tokens': 23, 'completion_tokens_details': {'reasoning_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=None, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:53.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='\"Warsaw\" in response.message'\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:53.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='response.stats.conn_duration < 3'\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:53.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Send hello' request=RequestBase(message='Hello!') response=None asserts=[AssertionEval(result=None, eval='response.stats.conn_duration < 3'), AssertionEval(result=None, eval='response.stats.tokens_response > 5'), AssertionEval(result=None, eval='\"Goodbye\" in response.message')] result=None\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:53.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Hello!'\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:54.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Hello! How can I assist you today?' stats=ResponseStats(tokens_request=9, tokens_response=9, tokens_total=18, conn_start_time=26157.807445583, conn_end_time=26158.546913272, conn_duration=0.7394676889998664) id='chatcmpl-AAfZxl5W2MGZ3hcTC3rk9pSBsM399' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Hello! How can I assist you today?', 'role': 'assistant', 'refusal': None}}] created=1727106473 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_e9627b5346' usage={'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'reasoning_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=None, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:54.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='response.stats.conn_duration < 3'\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:54.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='response.stats.tokens_response > 5'\u001b[0m\n",
      "\u001b[32m2024-09-23 17:47:54.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=False eval='\"Goodbye\" in response.message'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run test scenario\n",
    "executor.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestScenario(\n",
      "    steps=[\n",
      "        TestStep(\n",
      "            name='Write success in the response',\n",
      "            request=RequestBase(\n",
      "                message='Write success in the response',\n",
      "            ),\n",
      "            response=ResponseModel(\n",
      "                message='Success! How can I assist you further?',\n",
      "                stats=ResponseStats(\n",
      "                    tokens_request=12,\n",
      "                    tokens_response=9,\n",
      "                    tokens_total=21,\n",
      "                    conn_start_time=26156.204792279,\n",
      "                    conn_end_time=26157.269648521,\n",
      "                    conn_duration=1.0648562419992231,\n",
      "                ),\n",
      "                id='chatcmpl-AAfZwElri6ZErMaaIxv12EXRAzQ1S',\n",
      "                choices=[\n",
      "                    {\n",
      "                        'finish_reason': 'stop',\n",
      "                        'index': 0,\n",
      "                        'logprobs': None,\n",
      "                        'message': {\n",
      "                            'content': 'Success! How can I assist you further?',\n",
      "                            'role': 'assistant',\n",
      "                            'refusal': None,\n",
      "                        },\n",
      "                    },\n",
      "                ],\n",
      "                created=1727106472,\n",
      "                model='gpt-4o-mini-2024-07-18',\n",
      "                object='chat.completion',\n",
      "                system_fingerprint='fp_1bb46167f9',\n",
      "                usage={\n",
      "                    'completion_tokens': 9,\n",
      "                    'prompt_tokens': 12,\n",
      "                    'total_tokens': 21,\n",
      "                    'completion_tokens_details': {\n",
      "                        'reasoning_tokens': 0,\n",
      "                    },\n",
      "                },\n",
      "                config=EndpointConfig(\n",
      "                    kind='openai',\n",
      "                    url='',\n",
      "                    model='gpt-4o-mini',\n",
      "                    additional_headers={},\n",
      "                    provider=None,\n",
      "                    temperature=None,\n",
      "                    max_tokens=None,\n",
      "                    top_k=3,\n",
      "                    use_ranker=True,\n",
      "                    collection_name='default',\n",
      "                ),\n",
      "            ),\n",
      "            asserts=[],\n",
      "            result=True,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Check capital of Poland',\n",
      "            request=RequestBase(\n",
      "                message='What is the capital city of Poland?',\n",
      "            ),\n",
      "            response=ResponseModel(\n",
      "                message='The capital city of Poland is Warsaw.',\n",
      "                stats=ResponseStats(\n",
      "                    tokens_request=15,\n",
      "                    tokens_response=8,\n",
      "                    tokens_total=23,\n",
      "                    conn_start_time=26157.274484821,\n",
      "                    conn_end_time=26157.799638476,\n",
      "                    conn_duration=0.5251536550022138,\n",
      "                ),\n",
      "                id='chatcmpl-AAfZxBVNsg5hvd9TQE2bc5hMnFZIk',\n",
      "                choices=[\n",
      "                    {\n",
      "                        'finish_reason': 'stop',\n",
      "                        'index': 0,\n",
      "                        'logprobs': None,\n",
      "                        'message': {\n",
      "                            'content': 'The capital city of Poland is Warsaw.',\n",
      "                            'role': 'assistant',\n",
      "                            'refusal': None,\n",
      "                        },\n",
      "                    },\n",
      "                ],\n",
      "                created=1727106473,\n",
      "                model='gpt-4o-mini-2024-07-18',\n",
      "                object='chat.completion',\n",
      "                system_fingerprint='fp_1bb46167f9',\n",
      "                usage={\n",
      "                    'completion_tokens': 8,\n",
      "                    'prompt_tokens': 15,\n",
      "                    'total_tokens': 23,\n",
      "                    'completion_tokens_details': {\n",
      "                        'reasoning_tokens': 0,\n",
      "                    },\n",
      "                },\n",
      "                config=EndpointConfig(\n",
      "                    kind='openai',\n",
      "                    url='',\n",
      "                    model='gpt-4o-mini',\n",
      "                    additional_headers={},\n",
      "                    provider=None,\n",
      "                    temperature=None,\n",
      "                    max_tokens=None,\n",
      "                    top_k=3,\n",
      "                    use_ranker=True,\n",
      "                    collection_name='default',\n",
      "                ),\n",
      "            ),\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='\"Warsaw\" in response.message',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "            ],\n",
      "            result=True,\n",
      "        ),\n",
      "        TestStep(\n",
      "            name='Send hello',\n",
      "            request=RequestBase(\n",
      "                message='Hello!',\n",
      "            ),\n",
      "            response=ResponseModel(\n",
      "                message='Hello! How can I assist you today?',\n",
      "                stats=ResponseStats(\n",
      "                    tokens_request=9,\n",
      "                    tokens_response=9,\n",
      "                    tokens_total=18,\n",
      "                    conn_start_time=26157.807445583,\n",
      "                    conn_end_time=26158.546913272,\n",
      "                    conn_duration=0.7394676889998664,\n",
      "                ),\n",
      "                id='chatcmpl-AAfZxl5W2MGZ3hcTC3rk9pSBsM399',\n",
      "                choices=[\n",
      "                    {\n",
      "                        'finish_reason': 'stop',\n",
      "                        'index': 0,\n",
      "                        'logprobs': None,\n",
      "                        'message': {\n",
      "                            'content': 'Hello! How can I assist you today?',\n",
      "                            'role': 'assistant',\n",
      "                            'refusal': None,\n",
      "                        },\n",
      "                    },\n",
      "                ],\n",
      "                created=1727106473,\n",
      "                model='gpt-4o-mini-2024-07-18',\n",
      "                object='chat.completion',\n",
      "                system_fingerprint='fp_e9627b5346',\n",
      "                usage={\n",
      "                    'completion_tokens': 9,\n",
      "                    'prompt_tokens': 9,\n",
      "                    'total_tokens': 18,\n",
      "                    'completion_tokens_details': {\n",
      "                        'reasoning_tokens': 0,\n",
      "                    },\n",
      "                },\n",
      "                config=EndpointConfig(\n",
      "                    kind='openai',\n",
      "                    url='',\n",
      "                    model='gpt-4o-mini',\n",
      "                    additional_headers={},\n",
      "                    provider=None,\n",
      "                    temperature=None,\n",
      "                    max_tokens=None,\n",
      "                    top_k=3,\n",
      "                    use_ranker=True,\n",
      "                    collection_name='default',\n",
      "                ),\n",
      "            ),\n",
      "            asserts=[\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='response.stats.conn_duration < 3',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=True,\n",
      "                    eval='response.stats.tokens_response > 5',\n",
      "                ),\n",
      "                AssertionEval(\n",
      "                    result=False,\n",
      "                    eval='\"Goodbye\" in response.message',\n",
      "                ),\n",
      "            ],\n",
      "            result=False,\n",
      "        ),\n",
      "    ],\n",
      "    config=TestConfig(\n",
      "        endpoint_under_test=EndpointConfig(\n",
      "            kind='openai',\n",
      "            url='',\n",
      "            model='gpt-4o-mini',\n",
      "            additional_headers={},\n",
      "            provider=None,\n",
      "            temperature=None,\n",
      "            max_tokens=None,\n",
      "            top_k=3,\n",
      "            use_ranker=True,\n",
      "            collection_name='default',\n",
      "        ),\n",
      "        default_request=None,\n",
      "        eval_endpoint=None,\n",
      "    ),\n",
      "    result=False,\n",
      "    filename='scenario_openai.yaml',\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# NOTE RB: Maybe executor should copy the test scenario\n",
    "# Inspect updated test_scenario\n",
    "pprint(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step name: Write success in the response, Result: True\n",
      "Step name: Check capital of Poland, Result: True\n",
      "Step name: Send hello, Result: False\n"
     ]
    }
   ],
   "source": [
    "# We can inspect each test step separately and check its results\n",
    "for step in test_scenario.steps:\n",
    "    print(f\"Step name: {step.name}, Result: {step.result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step name: Write success in the response:\n",
      "\n",
      "------------\n",
      "Step name: Check capital of Poland:\n",
      "\n",
      "result=True eval='\"Warsaw\" in response.message'\n",
      "result=True eval='response.stats.conn_duration < 3'\n",
      "------------\n",
      "Step name: Send hello:\n",
      "\n",
      "result=True eval='response.stats.conn_duration < 3'\n",
      "result=True eval='response.stats.tokens_response > 5'\n",
      "result=False eval='\"Goodbye\" in response.message'\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: We can also inspect each assertion for each step separately\n",
    "for step in test_scenario.steps:\n",
    "    print(f\"Step name: {step.name}:\\n\")\n",
    "    for assertion in step.asserts:\n",
    "        print(assertion) \n",
    "    print(\"-\"*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
